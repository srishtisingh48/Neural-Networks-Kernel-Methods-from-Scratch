# -*- coding: utf-8 -*-
"""Copy of hw4-a4-matrix-completion-and-recommendation-systems.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CINy_4DCl_Kt-FGh9nKbA-lhbYAkx4M0

# Matrix Completion and Recommendation System
ðŸŽ¥ ðŸŽž ðŸŽ¬

## Information before starting

In this problem, we will be building a personalized movie recommendation system! To make these recommendations, we'll build on what we've learned in lecture about SVD and what we've practiced so far with Python and Python packages such as NumPy and PyTorch.

### Copying this Colab Notebook to your Google Drive

Since the course staff is the author of this notebook, you cannot make any lasting changes to it. You should make a copy of it to your Google Drive by clicking **File -> Save a Copy in Drive**.

### Problem Introduction

We will use the 100K MovieLens dataset available at https://grouplens.org/datasets/movielens/100k/ to estimate unknown user ratings given their previous ratings. Run the code block below to download the dataset.
"""

# @title loading dataset
!rm -rf ml-100k*
!wget https://files.grouplens.org/datasets/movielens/ml-100k.zip
!unzip ml-100k.zip
!mv ml-100k/u.data .

"""### Compute

This problem should not require using GPU. Since Google Colab will limit your GPU usage, we recommend saving your GPU quota for HW4 A3 and making sure that your runtime is set to CPU by going to **Runtime -> Change runtime type -> Select CPU under "Hardware accelerator"**.

### Submitting your assignment

Once you are done with the problem, make sure to put all of your necessary figures into your PDF submission. Then, download this notebook as a Python file (`.py`) by going to **File -> Download -> Download `.py`**. Rename this file as `hw4-a4.py` and upload to the Gradescope submission for HW4 code.

## Code: Setup

Let's start by importing the packages that we'll need to complete this problem.
"""

import csv
import numpy as np
from scipy.sparse.linalg import svds
import matplotlib.pyplot as plt
import torch

"""Now, let's load the 100K MovieLens data. If you have downloaded the `u.data` file and uploaded to the "Files" tab, the following code block will construct training and test sets for you. There are $m = 1682$ movies and $n = 943$ users in the dataset, and each user has rated at least 20 movies. The total dataset has 100,000 total ratings from all users, and our goal will be to estimate the unknown ratings that each user would assign to each movie. These ratings can then be used to recommend the "best" movies for each user!"""

data = []
with open('u.data') as csvfile:
    spamreader = csv.reader(csvfile, delimiter='\t')
    for row in spamreader:
        data.append([int(row[0])-1, int(row[1])-1, int(row[2])])
data = np.array(data)

num_observations = len(data)  # num_observations = 100,000
num_users = max(data[:,0])+1  # num_users = 943, indexed 0,...,942
num_items = max(data[:,1])+1  # num_items = 1682 indexed 0,...,1681

np.random.seed(1)
num_train = int(0.8*num_observations)
perm = np.random.permutation(data.shape[0])
train = data[perm[0:num_train],:]
test = data[perm[num_train::],:]

print(f"Successfully loaded 100K MovieLens dataset with",
      f"{len(train)} training samples and {len(test)} test samples")

"""For this problem, we will consider a matrix $R \in \mathbb{R}^{m \times n}$ where the entry $R_{i,j} \in \{1,...,5\}$ represents the $j$th user's rating on movie $i$. A higher value represents that the user is more staisfied with the movie.

## Code: Assignment

The rest is yours to code! We provide some scaffolding for your implementation, but feel free to modify it and implement however you would like to. You may use fundamental operators from `NumPy` and `PyTorch` in this problem, such as `numpy.linalg.lstsq, SVD, autograd`, etc., but you many not use any precooked algorithm from a package like `scikit-learn`.

### Part (a)

Our first estimator pools all users together and, for each movie, outputs as its prediction the average user rating of that movie in ``train``. That is, if $\mu \in \mathbb{R}^m$ is a vector where $\mu_i$ is the average rating of the users that rated the $i$-th movie. Write this estimator $\widehat{R}$ as a rank-one matrix.

Compute the estimate $\widehat{R}$. What is $\mathcal{E}_{\rm test} (\widehat{R})$ for this estimate?
"""

# Your code goes here. You should:

# 1. Compute estimate and
sums = np.zeros(num_items)
total = np.zeros(num_items)
for user, movie, rating in train:
    sums[movie] += rating
    total[movie] += 1
mu = np.zeros(num_items)
mu[total > 0] = sums[total > 0] / total[total > 0]
Rhat = np.tile(mu.reshape(-1, 1), num_users)

# 2. Evaluate test error
squared_errors = []
for user, movie, rating in test:
    squared_errors.append((Rhat[movie, user] - rating) ** 2)

print(f"error: {np.mean(squared_errors):.4f}")

"""### Part (b)
Allocate a matrix $\widetilde{R}_{i, j} \in \mathbb{R}^{m \times n}$ and set its entries equal to the known values in the training set, and $0$ otherwise.

Let $\widehat{R}^{(d)}$ be the best rank-$d$ approximation (in terms of squared error) approximation to $\widetilde{R}$. This is equivalent to computing the singular value decomposition (SVD) and using the top $d$ singular values. This learns a lower-dimensional vector representation for users and movies, assuming that each user would give a rating of $0$ to any movie they have not reviewed.

- For each $d = 1, 2, 5, 10, 20, 50$, compute the estimator $\widehat{R}^{(d)}$. We recommend using an efficient solver, such as ``scipy.sparse.linalg.svds``.
- Plot the average squared error of predictions on the training set and test set on a single plot, as a function of $d$.
"""

# Your code goes here
# Create the matrix R twiddle (\widetilde{R}).
r_twiddle = np.zeros((num_items, num_users))
for user, movie, rating in train:
    r_twiddle[movie, user] = rating

# Your code goes here
def construct_estimator(d, r_twiddle):
    (U, S, Vt) = svds(r_twiddle, k=d)
    return U[:, ::-1] @ np.diag(S[::-1]) @ Vt[::-1, :]


def get_error(Rhat, dataset):
  error = 0.0
  for user, movie, rating in dataset:
      prediction = Rhat[movie, user]
      error += (prediction - rating) ** 2
  return error / len(dataset)

# Your code goes here
# Evaluate train and test error for: d = 1, 2, 5, 10, 20, 50.
dataset = [1, 2, 5, 10, 20, 50]
train_errors = []
test_errors = []

for d in dataset:
    R_hat = construct_estimator(d, r_twiddle)
    train_err = get_error(R_hat, train)
    test_err = get_error(R_hat, test)
    train_errors.append(get_error(R_hat, train))
    test_errors.append(get_error(R_hat, test))
    print(f"d={d} | Train error: {train_err:.4f}, Test error: {test_err:.4f}")

# Your code goes here
# Plot both train and test error as a function of d on the same plot.
plt.figure(figsize=(8, 5))
plt.plot(dataset, train_errors, label="Train Error")
plt.plot(dataset, test_errors, label="Test Error")
plt.xlabel("Rank d")
plt.ylabel("Mean Squared Error")
plt.title("SVD Mean Squared Error vs. Rank d")
plt.legend()
plt.show()

"""### Part (c)
Replacing all missing values by a constant may impose strong and potentially incorrect assumptions on the unobserved entries of $R$. A more reasonable choice is to minimize the mean squared error (MSE) only on rated movies. Define a loss function:
$$
\mathcal{L} \left( \{u_i\}_{i=1}^m, \{v_j\}_{j=1}^n \right) :=
\sum_{(i, j, R_{i, j}) \in {\rm train}} (\langle u_i,v_j\rangle - R_{i,j})^2 +
\lambda \sum_{i=1}^m \|u_i\|_2^2 +
\lambda \sum_{j=1}^n \|v_j\|_2^2
$$
where $\lambda > 0$ is the regularization coefficient. We will implement algorithms to learn vector representations by minimizing the above loss. You may need to tune $\lambda$ and $\sigma$ to optimize the loss.

Implement alternating minimization and plot the MSE of ``train`` and ``test`` for $d \in \{1, 2, 5, 10, 20, 50\}$.

*Note: we define the loss function here as the sum of squared errors; be careful to calculate and plot the mean squared error for your results*
"""

# Your code goes here. You are welcome to change the parameter lists and/or write new functions to complete this part of the assignment.
# In particular, you will likely also want to use R twiddle, and you may want to create global data structures to store observed entries.
# These global data structures might look like mappings of users to the movies they've reviewed, and of movies to the users who have reviewed that movie.

def closed_form_u(V, U, R, user_to_rated_items, l):
    d = V.shape[1]
    U = np.zeros((len(user_to_rated_items), d))
    I = np.eye(d)
    for u in range(len(user_to_rated_items)):
        rated = user_to_rated_items[u]
        if not user_to_rated_items[u]:
            continue
        movies = []
        for m, i in rated:
            movies.append(m)
        ratings = np.array([R[u, m] for m in movies])
        V_m = V[movies, :]
        U[u] = np.linalg.solve(V_m.T @ V_m + l * I, V_m.T @ ratings)
    return U

def closed_form_v(V, U, R, item_to_rated, l):
    d = U.shape[1]
    V = np.zeros((len(item_to_rated), d))
    I = np.eye(d)
    for m in range(len(item_to_rated)):
        rated = item_to_rated[m]
        if not rated:
            continue
        users = []
        for u, i in rated:
            users.append(u)
        ratings = np.array([R[u, m] for u in users])
        U_u = U[users, :]
        V[m] = np.linalg.solve(U_u.T @ U_u + l * I, U_u.T @ ratings)
    return V

def construct_alternating_estimator(d, train_data, l=10.0, delta=1e-3, sigma=0.1, max_iters=20):
    R = np.zeros((num_users, num_items))
    user_to_rated_items = {}
    for u in range(num_users):
        user_to_rated_items[u] = []

    item_to_rated = {}
    for m in range(num_items):
        item_to_rated[m] = []
    for u, m, r in train_data:
        R[u, m] = r
        user_to_rated_items[u].append((m, r))
        item_to_rated[m].append((u, r))
    U = np.random.normal(0, sigma, (num_users, d))
    V = np.random.normal(0, sigma, (num_items, d))
    previous = float("inf")
    for i in range(max_iters):
        U = closed_form_u(V, U, R, user_to_rated_items, l)
        V = closed_form_v(V, U, R, item_to_rated, l)
        loss = compute_mse(U, V, train_data)
        if abs(previous - loss) < delta:
            break
        previous = loss
    return U, V

# Your code goes here
# Any additional functions that you may write to help implement alternating minimization.
def compute_mse(U, V, data):
    return np.mean([(np.dot(U[u], V[m]) - r) ** 2 for u, m, r in data])

# Your code goes here
# Evaluate train and test error for: d = 1, 2, 5, 10, 20, 50.
def alternating_minimization(train, test):
    dataset = [1, 2, 5, 10, 20, 50]
    train_mses = []
    test_mses = []
    for d in dataset:
        (U, V) = construct_alternating_estimator(d, train)
        train_mse = compute_mse(U, V, train)
        test_mse = compute_mse(U, V, test)
        train_mses.append(train_mse)
        test_mses.append(test_mse)
        print(f"Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}")
    return dataset, train_mses, test_mses

# Your code goes here
# Plot both train and test error as a function of d on the same plot.
ds, train_mses, test_mses = alternating_minimization(train, test)
plt.figure(figsize=(8, 5))
plt.plot(ds, train_mses, label="Train MSE")
plt.plot(ds, test_mses, label="Test MSE")
plt.xlabel("d")
plt.ylabel("Mean Squared Error")
plt.title("Alternating Minimization")
plt.legend()
plt.grid(True)
plt.show()